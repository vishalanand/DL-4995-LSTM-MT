{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "import copy\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "def load_train_data():\n",
    "    X_train = None\n",
    "    Y_train = None\n",
    "    for i in range(1, 6):\n",
    "        pickleFile = unpickle('cifar-10-batches-py/data_batch_{}'.format(i))\n",
    "        dataX = pickleFile[b'data']\n",
    "        dataY = pickleFile[b'labels']\n",
    "        if type(X_train) is np.ndarray:\n",
    "            X_train = np.concatenate((X_train, dataX))\n",
    "            Y_train = np.concatenate((Y_train, dataY))\n",
    "        else:\n",
    "            X_train = dataX\n",
    "            Y_train = dataY\n",
    "\n",
    "    Y_train = Y_train.reshape(Y_train.shape[0], 1)\n",
    "\n",
    "    return X_train.T, Y_train.T\n",
    "\n",
    "def load_test_data():\n",
    "    X_test = None\n",
    "    Y_test = None\n",
    "    pickleFile = unpickle('cifar-10-batches-py/test_batch')\n",
    "    dataX = pickleFile[b'data']\n",
    "    dataY = pickleFile[b'labels']\n",
    "    if type(X_test) is np.ndarray:\n",
    "        X_test = np.concatenate((X_test, dataX))\n",
    "        Y_test = np.concatenate((Y_test, dataY))\n",
    "    else:\n",
    "        X_test = np.array(dataX)\n",
    "        Y_test = np.array(dataY)\n",
    "\n",
    "    Y_test = Y_test.reshape(Y_test.shape[0], 1)\n",
    "\n",
    "    return X_test.T, Y_test.T\n",
    "\n",
    "def train_test_split(X_train, Y_train):\n",
    "    msk = np.random.rand(Y_train.shape[1]) < 0.8\n",
    "    X_Train = X_train[:,msk]  \n",
    "    X_val = X_train[:,~msk]\n",
    "    Y_Train = Y_train[:,msk]  \n",
    "    Y_val = Y_train[:,~msk]\n",
    "\n",
    "    return X_Train, Y_Train, X_val, Y_val\n",
    "\n",
    "def get_batch(X, Y, batch_size):\n",
    "    n_batches = int(Y.shape[1]/batch_size)\n",
    "    idx = np.arange(Y.shape[1])\n",
    "    np.random.shuffle(idx)\n",
    "    \n",
    "    ret = []\n",
    "    for i in range(0,int(len(idx)/batch_size)):\n",
    "        chunk = idx[i*batch_size:(i+1)*batch_size]\n",
    "        ret.append(chunk)\n",
    "    mini = ret\n",
    "    \n",
    "    return mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv1 = nn.Conv2d(3, 64, 5)\n",
    "        self.conv2 = nn.Conv2d(64, 350, 5)\n",
    "        #self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(350 * 5 * 5, 1200)\n",
    "        self.fc1_2 = nn.Linear(1200, 400)\n",
    "        #self.fc1_3 = nn.Linear(1000, 400)\n",
    "        self.fc2 = nn.Linear(400, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 350 * 5 * 5)\n",
    "        #x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc1_2(x))\n",
    "\n",
    "        #x = F.relu(self.fc1_3(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,Y_train = load_train_data()\n",
    "X_test, Y_test = load_test_data()\n",
    "X_Train, Y_Train, X_Val, Y_Val = train_test_split(X_train,Y_train)\n",
    "\n",
    "minn = np.min(X_Train, axis=1,keepdims=True)\n",
    "maxx = np.max(X_Train, axis=1,keepdims=True)\n",
    "X_Train= (X_Train - minn)/(maxx-minn)\n",
    "X_Val= (X_Val - minn)/(maxx-minn)\n",
    "X_Test= (X_test - minn)/(maxx-minn)\n",
    "\n",
    "net = Net().cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "optimizer = optim.RMSprop(net.parameters(), lr=0.0001)#, momentum=0.9)\n",
    "\n",
    "def test_acc():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    idx = get_batch(X_Test, Y_test, batch_size_len)\n",
    "\n",
    "    for idx_list in idx:\n",
    "        x_batch_test = [X_Test.T[index] for index in idx_list]\n",
    "        y_batch_test = [Y_test.T[index] for index in idx_list]\n",
    "        x_batch_test = np.asarray(x_batch_test)\n",
    "        y_batch_test1 = np.asarray(y_batch_test)\n",
    "        y_batch_onehot_test = get_one_hot(y_batch_test1,10)\n",
    "        label_tensor_test = torch.from_numpy(y_batch_onehot_test)\n",
    "        test_labels = Variable(label_tensor_test.cuda()).long()\n",
    "        true_labels = torch.max(test_labels,1)[1]\n",
    "        x_batch_test = x_batch_test.reshape(x_batch_test.shape[0],3,32,32)    \n",
    "        input_tensor_test = torch.from_numpy(x_batch_test)\n",
    "        images = Variable(input_tensor_test.cuda()).float()\n",
    "        outputs = net(images)\n",
    "        ##### loss = criterion(outputs, torch.max(labels, 1)[1])\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == true_labels.data.long()).sum()\n",
    "    \n",
    "    return (100 * correct / total)\n",
    "\n",
    "def get_one_hot(targets, nb_classes):\n",
    "    return np.eye(nb_classes)[np.array(targets).reshape(-1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n",
      "Epoch 1 Count: 300 loss: 4.255185467004776 \t 249 loss_val: 8.396954491734505 Test: 52.01\n",
      "Epoch 2 Count: 300 loss: 3.3210474252700806 \t 249 loss_val: 8.28308934867382 Test: 51.25\n",
      "Epoch 3 Count: 300 loss: 2.808590777218342 \t 249 loss_val: 6.674414798617363 Test: 61.53\n",
      "Epoch 4 Count: 300 loss: 2.5398551270365717 \t 249 loss_val: 6.376598713546992 Test: 63.56\n",
      "Epoch 5 Count: 300 loss: 2.361496549844742 \t 249 loss_val: 5.786540494859219 Test: 67.3\n",
      "Epoch 6 Count: 300 loss: 2.0495502799749374 \t 249 loss_val: 6.092182673513889 Test: 66.14\n",
      "Epoch 7 Count: 300 loss: 1.831852240115404 \t 249 loss_val: 5.69258571639657 Test: 68.09\n",
      "Epoch 8 Count: 300 loss: 1.5314211934804915 \t 249 loss_val: 5.469872649013996 Test: 69.46\n",
      "Epoch 9 Count: 300 loss: 1.3816835641860963 \t 249 loss_val: 5.731236854195595 Test: 68.79\n",
      "Epoch 10 Count: 300 loss: 1.1743870101869107 \t 249 loss_val: 5.4303813353180885 Test: 70.37\n",
      "Epoch 11 Count: 300 loss: 0.9524250470101834 \t 249 loss_val: 5.738679330796003 Test: 70.66\n",
      "Epoch 12 Count: 300 loss: 0.7290793279185891 \t 249 loss_val: 6.060081222653389 Test: 71.08\n",
      "Epoch 13 Count: 300 loss: 0.5405447712168098 \t 249 loss_val: 6.264443264901638 Test: 71.51\n",
      "Epoch 14 Count: 300 loss: 0.4234976353123784 \t 249 loss_val: 6.601436275243759 Test: 72.2\n",
      "Epoch 15 Count: 300 loss: 0.27067211070097985 \t 249 loss_val: 7.352132000774145 Test: 70.64\n",
      "Epoch 16 Count: 300 loss: 0.20481262197718025 "
     ]
    }
   ],
   "source": [
    "trainacc=[]\n",
    "valacc=[]\n",
    "testacc = []\n",
    "traincost = []\n",
    "valcost = []\n",
    "epoch_count = 200\n",
    "num = epoch_count\n",
    "batch_size_len = 40 \n",
    "\n",
    "train = \"\"\n",
    "print(\"Start\", flush=True)\n",
    "for epoch in range(num):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    running_loss_val = 0.0\n",
    "    train_accuracy = 0.0\n",
    "    val_accuracy = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    #print(epoch, flush=True)\n",
    "    abcd_train = get_batch(X_Train, Y_Train,batch_size_len)\n",
    "    for i, idx in enumerate(abcd_train, 0):\n",
    "        # get the inputs\n",
    "        x_batch = [X_Train.T[index] for index in idx]\n",
    "        y_batch = [Y_Train.T[index] for index in idx]\n",
    "        x_batch = np.asarray(x_batch)\n",
    "        y_batch = np.asarray(y_batch)\n",
    "        y_batch_onehot = get_one_hot(y_batch,10)\n",
    "        x_batch = x_batch.reshape(x_batch.shape[0],3,32,32)\n",
    "        input_tensor = torch.from_numpy(x_batch)\n",
    "        label_tensor = torch.from_numpy(y_batch_onehot)\n",
    "        inputs = Variable(input_tensor.cuda()).float()\n",
    "        labels = Variable(label_tensor.cuda()).long()\n",
    "        true_labels = torch.max(labels,1)[1]\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, torch.max(labels, 1)[1])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.data[0]\n",
    "        if i % 100 == 99:    # print every 2000 mini-batches\n",
    "            if(i+1 == 300):\n",
    "                #print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 100), flush=True)\n",
    "                #train = train + str(epoch + 1) + \", \" + str(i + 1) + \", loss=\" + str(running_loss / 100) + \"; \"\n",
    "                print(\"Epoch\", epoch + 1, \"Count:\", i + 1, \"loss:\", running_loss/(batch_size_len*1.0), end=' ')\n",
    "            traincost.append(running_loss / (batch_size_len*1.0))\n",
    "            running_loss = 0.0\n",
    "            \n",
    "        _, predicted_train = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted_train == true_labels.data.long()).sum()\n",
    "    train_accuracy = 100 * correct_train / (total_train + 0.0001)\n",
    "        \n",
    "    abcd = get_batch(X_Val, Y_Val,batch_size_len)\n",
    "#     for l in abcd:\n",
    "#         print(len(l), end=' ')\n",
    "#     print(\"\")\n",
    "    for i, idx in enumerate(abcd, 0):\n",
    "        # get the inputs\n",
    "        x_batch_val = [X_Val.T[index] for index in idx]\n",
    "        y_batch_val = [Y_Val.T[index] for index in idx]\n",
    "        x_batch_val = np.asarray(x_batch_val)\n",
    "        y_batch_val = np.asarray(y_batch_val)\n",
    "        y_batch_onehot_val = get_one_hot(y_batch_val,10)\n",
    "        x_batch_val = x_batch_val.reshape(x_batch_val.shape[0],3,32,32)\n",
    "        input_tensor_val = torch.from_numpy(x_batch_val)\n",
    "        label_tensor_val = torch.from_numpy(y_batch_onehot_val)\n",
    "        inputs_val = Variable(input_tensor_val.cuda()).float()\n",
    "        labels_val = Variable(label_tensor_val.cuda()).long()\n",
    "        true_labels_val = torch.max(labels_val,1)[1]\n",
    "        optimizer.zero_grad()\n",
    "        outputs_val = net(inputs_val)\n",
    "        loss_val = criterion(outputs_val, torch.max(labels_val, 1)[1])\n",
    "        running_loss_val += loss_val.data[0]\n",
    "#         if(i > 97):\n",
    "#             print(\"Hello\", i)\n",
    "        if i == len(abcd) - 1:    # print every 2000 mini-batches\n",
    "            test_accuracy = test_acc()\n",
    "            print(\"\\t\", i, \"loss_val:\", running_loss_val/(batch_size_len*1.0), \"Test:\", test_accuracy)\n",
    "            valcost.append(running_loss_val/(batch_size_len*1.0))\n",
    "            running_loss_val = 0.0\n",
    "        \n",
    "        _, predicted_val = torch.max(outputs_val.data, 1)\n",
    "        total_val += labels_val.size(0)\n",
    "        correct_val += (predicted_val == true_labels_val.data.long()).sum()\n",
    "    val_accuracy = 100 * correct_val / total_val\n",
    "    trainacc.append(train_accuracy)\n",
    "    valacc.append(val_accuracy)\n",
    "    testacc.append(test_accuracy)\n",
    "    if(val_accuracy>=80.2):\n",
    "        print(\"Target Val Accuracy reached. Breaking\", flush=True)\n",
    "        break\n",
    "    \n",
    "print('Finished Training', flush=True)\n",
    "print('Training Accuracy ',trainacc, flush=True)\n",
    "print('validation Accuracy',valacc, flush=True)\n",
    "print(\"Test:\", test_acc(), flush=True)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(trainacc, 'r', label ='train-accuracy')\n",
    "plt.plot(valacc, 'b', label = 'val-accuracy')\n",
    "plt.plot(testacc, 'g', label = 'test-accuracy')\n",
    "plt.legend()\n",
    "plt.savefig(\"test.png\")\n",
    "\n",
    "\n",
    "x_t = []\n",
    "x_v = []\n",
    "\n",
    "for i in range(len(traincost)):\n",
    "    x_t.append((i+1)*batch_size_len)\n",
    "\n",
    "for i in range(len(valcost)):\n",
    "    x_v.append((i+1)*batch_size_len)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x_t,traincost, 'b', label = 'traincost')\n",
    "plt.plot(x_v,valcost, 'r', label = 'valcost')\n",
    "plt.legend()\n",
    "plt.savefig(\"test1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(valacc)\n",
    "print(testacc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "import copy\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "def load_train_data():\n",
    "    X_train = None\n",
    "    Y_train = None\n",
    "    for i in range(1, 6):\n",
    "        pickleFile = unpickle('cifar-10-batches-py/data_batch_{}'.format(i))\n",
    "        dataX = pickleFile[b'data']\n",
    "        dataY = pickleFile[b'labels']\n",
    "        if type(X_train) is np.ndarray:\n",
    "            X_train = np.concatenate((X_train, dataX))\n",
    "            Y_train = np.concatenate((Y_train, dataY))\n",
    "        else:\n",
    "            X_train = dataX\n",
    "            Y_train = dataY\n",
    "\n",
    "    Y_train = Y_train.reshape(Y_train.shape[0], 1)\n",
    "\n",
    "    return X_train.T, Y_train.T\n",
    "\n",
    "def load_test_data():\n",
    "    X_test = None\n",
    "    Y_test = None\n",
    "    pickleFile = unpickle('cifar-10-batches-py/test_batch')\n",
    "    dataX = pickleFile[b'data']\n",
    "    dataY = pickleFile[b'labels']\n",
    "    if type(X_test) is np.ndarray:\n",
    "        X_test = np.concatenate((X_test, dataX))\n",
    "        Y_test = np.concatenate((Y_test, dataY))\n",
    "    else:\n",
    "        X_test = np.array(dataX)\n",
    "        Y_test = np.array(dataY)\n",
    "\n",
    "    Y_test = Y_test.reshape(Y_test.shape[0], 1)\n",
    "\n",
    "    return X_test.T, Y_test.T\n",
    "\n",
    "def train_test_split(X_train, Y_train):\n",
    "    msk = np.random.rand(Y_train.shape[1]) < 0.8\n",
    "    X_Train = X_train[:,msk]  \n",
    "    X_val = X_train[:,~msk]\n",
    "    Y_Train = Y_train[:,msk]  \n",
    "    Y_val = Y_train[:,~msk]\n",
    "\n",
    "    return X_Train, Y_Train, X_val, Y_val\n",
    "\n",
    "def get_batch1(X, Y, batch_size):\n",
    "    n_batches = Y.shape[1]/batch_size\n",
    "    idx = np.arange(Y.shape[1])\n",
    "    np.random.shuffle(idx)\n",
    "    mini = np.array_split(idx, n_batches)\n",
    "\n",
    "    return mini\n",
    "\n",
    "def get_batch(X, Y, batch_size):\n",
    "    n_batches = int(Y.shape[1]/batch_size)\n",
    "    idx = np.arange(Y.shape[1])\n",
    "    np.random.shuffle(idx)\n",
    "    \n",
    "    ret = []\n",
    "    for i in range(0,int(len(idx)/batch_size)):\n",
    "        chunk = idx[i*batch_size:(i+1)*batch_size]\n",
    "        ret.append(chunk)\n",
    "    mini = ret\n",
    "    \n",
    "    return mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv1 = nn.Conv2d(3, 64, 5)\n",
    "        self.conv2 = nn.Conv2d(64, 256, 5)\n",
    "        self.fc1 = nn.Linear(256 * 5 * 5, 400)\n",
    "        self.fc2 = nn.Linear(400, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 256 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,Y_train = load_train_data()\n",
    "X_test, Y_test = load_test_data()\n",
    "X_Train, Y_Train, X_Val, Y_Val = train_test_split(X_train,Y_train)\n",
    "\n",
    "minn = np.min(X_Train, axis=1,keepdims=True)\n",
    "maxx = np.max(X_Train, axis=1,keepdims=True)\n",
    "X_Train= (X_Train - minn)/(maxx-minn)\n",
    "X_Val= (X_Val - minn)/(maxx-minn)\n",
    "X_Test= (X_test - minn)/(maxx-minn)\n",
    "\n",
    "net = Net().cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "def test_acc():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    idx = get_batch(X_Test, Y_test, batch_size_len)\n",
    "\n",
    "    for idx_list in idx:\n",
    "        x_batch_test = [X_Test.T[index] for index in idx_list]\n",
    "        y_batch_test = [Y_test.T[index] for index in idx_list]\n",
    "        x_batch_test = np.asarray(x_batch_test)\n",
    "        y_batch_test1 = np.asarray(y_batch_test)\n",
    "        y_batch_onehot_test = get_one_hot(y_batch_test1,10)\n",
    "        label_tensor_test = torch.from_numpy(y_batch_onehot_test)\n",
    "        test_labels = Variable(label_tensor_test.cuda()).long()\n",
    "        true_labels = torch.max(test_labels,1)[1]\n",
    "        x_batch_test = x_batch_test.reshape(x_batch_test.shape[0],3,32,32)    \n",
    "        input_tensor_test = torch.from_numpy(x_batch_test)\n",
    "        images = Variable(input_tensor_test.cuda()).float()\n",
    "        outputs = net(images)\n",
    "        ##### loss = criterion(outputs, torch.max(labels, 1)[1])\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == true_labels.data.long()).sum()\n",
    "    \n",
    "    return (100 * correct / total)\n",
    "\n",
    "def get_one_hot(targets, nb_classes):\n",
    "    return np.eye(nb_classes)[np.array(targets).reshape(-1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n",
      "Epoch 1 Count: 300 loss: 4.550927605628967 \t 197 loss_val: 8.068672590255737 Test: 50.12\n",
      "Epoch 2 Count: 300 loss: 3.892080979347229 \t 197 loss_val: 7.025907588005066 Test: 73.8\n",
      "Epoch 3 Count: 300 loss: 3.4492373275756836 \t 197 loss_val: 6.270764124393463 Test: 86.9\n",
      "Epoch 4 Count: 300 loss: 3.126327736377716 \t 197 loss_val: 5.826485526561737 Test: 96.1\n",
      "Epoch 5 Count: 300 loss: 2.914099440574646 \t 197 loss_val: 5.568549592494964 Test: 98.8\n",
      "Epoch 6 Count: 300 loss: 2.7545438134670257 \t 197 loss_val: 5.4416193413734435 Test: 102.82\n",
      "Epoch 7 Count: 300 loss: 2.652080501317978 \t 197 loss_val: 5.160427441596985 Test: 109.9\n",
      "Epoch 8 Count: 300 loss: 2.547520143985748 \t 197 loss_val: 5.06253956079483 Test: 109.6\n",
      "Epoch 9 Count: 300 loss: 2.4246921920776368 \t 197 loss_val: 4.8188308501243595 Test: 114.96\n",
      "Epoch 10 Count: 300 loss: 2.3305360066890715 \t 197 loss_val: 4.829622093439102 Test: 116.38\n",
      "Epoch 11 Count: 300 loss: 2.2133016061782835 \t 197 loss_val: 4.532050819396972 Test: 118.92\n",
      "Epoch 12 Count: 300 loss: 2.146098815202713 \t 197 loss_val: 4.407829806804657 Test: 121.74\n",
      "Epoch 13 Count: 300 loss: 2.0259077060222626 \t 197 loss_val: 4.747604205608368 Test: 116.6\n",
      "Epoch 14 Count: 300 loss: 1.9601995539665222 \t 197 loss_val: 4.226679861545563 Test: 126.02\n",
      "Epoch 15 Count: 300 loss: 1.907526148557663 \t 197 loss_val: 4.070948110818863 Test: 127.54\n",
      "Epoch 16 Count: 300 loss: 1.8830981850624084 \t 197 loss_val: 3.9759120774269103 Test: 129.28\n",
      "Epoch 17 Count: 300 loss: 1.7507583439350127 \t 197 loss_val: 3.942912902832031 Test: 131.0\n",
      "Epoch 18 Count: 300 loss: 1.6407839345932007 \t 197 loss_val: 3.7765332329273225 Test: 133.58\n",
      "Epoch 19 Count: 300 loss: 1.5870957911014556 \t 197 loss_val: 3.751204878091812 Test: 134.98\n",
      "Epoch 20 Count: 300 loss: 1.5109069651365281 \t 197 loss_val: 3.798622624874115 Test: 133.68\n",
      "Epoch 21 Count: 300 loss: 1.4335696142911911 \t 197 loss_val: 3.636740751862526 Test: 137.04\n",
      "Epoch 22 Count: 300 loss: 1.3481460988521576 \t 197 loss_val: 3.6830001586675642 Test: 136.3\n",
      "Epoch 23 Count: 300 loss: 1.2518524539470672 \t 197 loss_val: 3.6378491711616516 Test: 137.68\n",
      "Epoch 24 Count: 300 loss: 1.2271366143226623 \t 197 loss_val: 3.5603762555122374 Test: 137.64\n",
      "Epoch 25 Count: 300 loss: 1.0893947142362594 \t 197 loss_val: 3.647474314570427 Test: 138.84\n",
      "Epoch 26 Count: 300 loss: 1.0343877583742143 \t 197 loss_val: 3.6839984327554705 Test: 137.08\n",
      "Epoch 27 Count: 300 loss: 0.9656847873330117 \t 197 loss_val: 3.893024594783783 Test: 135.16\n",
      "Epoch 28 Count: 300 loss: 0.9008035942912102 \t 197 loss_val: 3.795875707864761 Test: 138.0\n",
      "Epoch 29 Count: 300 loss: 0.8096105322241783 \t 197 loss_val: 3.781006626486778 Test: 137.9\n",
      "Epoch 30 Count: 300 loss: 0.733726613521576 \t 197 loss_val: 3.762698730826378 Test: 138.1\n",
      "Epoch 31 Count: 300 loss: 0.6493597128987312 \t 197 loss_val: 4.026939038038254 Test: 136.88\n",
      "Epoch 32 Count: 300 loss: 0.57533083319664 \t 197 loss_val: 4.292318810224533 Test: 134.92\n",
      "Epoch 33 Count: 300 loss: 0.5247445566952229 \t 197 loss_val: 4.258189318776131 Test: 134.18\n",
      "Epoch 34 Count: 300 loss: 0.4635329605638981 \t 197 loss_val: 4.2237863135337825 Test: 138.08\n",
      "Epoch 35 Count: 300 loss: 0.4014969880878925 \t 197 loss_val: 4.503125270009041 Test: 135.34\n",
      "Epoch 36 Count: 300 loss: 0.33616226375103 \t 197 loss_val: 4.567047572135925 Test: 138.1\n",
      "Epoch 37 Count: 300 loss: 0.255215729624033 \t 197 loss_val: 4.62895664691925 Test: 137.74\n",
      "Epoch 38 Count: 300 loss: 0.22667665243148805 \t 197 loss_val: 4.801746152639389 Test: 136.76\n",
      "Epoch 39 Count: 300 loss: 0.18252443380653857 \t 197 loss_val: 4.811659429073334 Test: 138.34\n",
      "Epoch 40 Count: 300 loss: 0.13489029448479414 \t 197 loss_val: 4.843263641595841 Test: 137.34\n",
      "Epoch 41 Count: 300 loss: 0.1136114664748311 \t 197 loss_val: 5.11363132417202 Test: 137.52\n",
      "Epoch 42 Count: 300 loss: 0.08550050323829055 \t 197 loss_val: 5.193208421468735 Test: 138.24\n",
      "Epoch 43 Count: 300 loss: 0.07445768794044852 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-caf1118d7a45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0minput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mlabel_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_batch_onehot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mtrue_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_cuda\u001b[0;34m(self, device, async)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mnew_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainacc=[]\n",
    "valacc=[]\n",
    "testacc = []\n",
    "traincost = []\n",
    "valcost = []\n",
    "epoch_count = 200\n",
    "num = epoch_count\n",
    "batch_size_len = 50 \n",
    "\n",
    "train = \"\"\n",
    "print(\"Start\", flush=True)\n",
    "for epoch in range(num):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    running_loss_val = 0.0\n",
    "    train_accuracy = 0.0\n",
    "    val_accuracy = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    #print(epoch, flush=True)\n",
    "    abcd_train = get_batch(X_Train, Y_Train,batch_size_len)\n",
    "    for i, idx in enumerate(abcd_train, 0):\n",
    "        # get the inputs\n",
    "        x_batch = [X_Train.T[index] for index in idx]\n",
    "        y_batch = [Y_Train.T[index] for index in idx]\n",
    "        x_batch = np.asarray(x_batch)\n",
    "        y_batch = np.asarray(y_batch)\n",
    "        y_batch_onehot = get_one_hot(y_batch,10)\n",
    "        x_batch = x_batch.reshape(x_batch.shape[0],3,32,32)\n",
    "        input_tensor = torch.from_numpy(x_batch)\n",
    "        label_tensor = torch.from_numpy(y_batch_onehot)\n",
    "        inputs = Variable(input_tensor.cuda()).float()\n",
    "        labels = Variable(label_tensor.cuda()).long()\n",
    "        true_labels = torch.max(labels,1)[1]\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, torch.max(labels, 1)[1])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.data[0]\n",
    "        if i % 100 == 99:    # print every 2000 mini-batches\n",
    "            if(i+1 == 300):\n",
    "                #print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 100), flush=True)\n",
    "                #train = train + str(epoch + 1) + \", \" + str(i + 1) + \", loss=\" + str(running_loss / 100) + \"; \"\n",
    "                print(\"Epoch\", epoch + 1, \"Count:\", i + 1, \"loss:\", running_loss/(batch_size_len*1.0), end=' ')\n",
    "            traincost.append(running_loss / (batch_size_len*1.0))\n",
    "            running_loss = 0.0\n",
    "            \n",
    "        _, predicted_train = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted_train == true_labels.data.long()).sum()\n",
    "    train_accuracy = 100 * correct_train / (total_train + 0.0001)\n",
    "        \n",
    "    abcd = get_batch(X_Val, Y_Val,batch_size_len)\n",
    "#     for l in abcd:\n",
    "#         print(len(l), end=' ')\n",
    "#     print(\"\")\n",
    "    for i, idx in enumerate(abcd, 0):\n",
    "        # get the inputs\n",
    "        x_batch_val = [X_Val.T[index] for index in idx]\n",
    "        y_batch_val = [Y_Val.T[index] for index in idx]\n",
    "        x_batch_val = np.asarray(x_batch_val)\n",
    "        y_batch_val = np.asarray(y_batch_val)\n",
    "        y_batch_onehot_val = get_one_hot(y_batch_val,10)\n",
    "        x_batch_val = x_batch_val.reshape(x_batch_val.shape[0],3,32,32)\n",
    "        input_tensor_val = torch.from_numpy(x_batch_val)\n",
    "        label_tensor_val = torch.from_numpy(y_batch_onehot_val)\n",
    "        inputs_val = Variable(input_tensor_val.cuda()).float()\n",
    "        labels_val = Variable(label_tensor_val.cuda()).long()\n",
    "        true_labels_val = torch.max(labels_val,1)[1]\n",
    "        optimizer.zero_grad()\n",
    "        outputs_val = net(inputs_val)\n",
    "        loss_val = criterion(outputs_val, torch.max(labels_val, 1)[1])\n",
    "        running_loss_val += loss_val.data[0]\n",
    "#         if(i > 97):\n",
    "#             print(\"Hello\", i)\n",
    "        if i == len(abcd) - 1:    # print every 2000 mini-batches\n",
    "            test_accuracy = test_acc()\n",
    "            print(\"\\t\", i, \"loss_val:\", running_loss_val/(batch_size_len*1.0), \"Test:\", test_accuracy)\n",
    "            valcost.append(running_loss_val/(batch_size_len*1.0))\n",
    "            running_loss_val = 0.0\n",
    "        \n",
    "        _, predicted_val = torch.max(outputs_val.data, 1)\n",
    "        total_val += labels_val.size(0)\n",
    "        correct_val += (predicted_val == true_labels_val.data.long()).sum()\n",
    "    val_accuracy = 100 * correct_val / total_val\n",
    "    trainacc.append(train_accuracy)\n",
    "    valacc.append(val_accuracy)\n",
    "    testacc.append(test_accuracy)\n",
    "    if(val_accuracy>=70.2):\n",
    "        print(\"Target Val Accuracy reached. Breaking\", flush=True)\n",
    "        break\n",
    "    \n",
    "print('Finished Training', flush=True)\n",
    "print('Training Accuracy ',trainacc, flush=True)\n",
    "print('validation Accuracy',valacc, flush=True)\n",
    "print(\"Test:\", test_acc(), flush=True)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(trainacc, 'r', label ='train-accuracy')\n",
    "plt.plot(valacc, 'b', label = 'val-accuracy')\n",
    "plt.plot(testacc, 'g', label = 'test-accuracy')\n",
    "plt.legend()\n",
    "plt.savefig(\"test.png\")\n",
    "\n",
    "\n",
    "x_t = []\n",
    "x_v = []\n",
    "\n",
    "for i in range(len(traincost)):\n",
    "    x_t.append((i+1)*batch_size_len)\n",
    "\n",
    "for i in range(len(valcost)):\n",
    "    x_v.append((i+1)*batch_size_len)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x_t,traincost, 'b', label = 'traincost')\n",
    "plt.plot(x_v,valcost, 'r', label = 'valcost')\n",
    "plt.legend()\n",
    "plt.savefig(\"test1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

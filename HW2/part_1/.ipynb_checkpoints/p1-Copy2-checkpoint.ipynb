{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "import copy\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "def load_train_data():\n",
    "    X_train = None\n",
    "    Y_train = None\n",
    "    for i in range(1, 6):\n",
    "        pickleFile = unpickle('cifar-10-batches-py/data_batch_{}'.format(i))\n",
    "        dataX = pickleFile[b'data']\n",
    "        dataY = pickleFile[b'labels']\n",
    "        if type(X_train) is np.ndarray:\n",
    "            X_train = np.concatenate((X_train, dataX))\n",
    "            Y_train = np.concatenate((Y_train, dataY))\n",
    "        else:\n",
    "            X_train = dataX\n",
    "            Y_train = dataY\n",
    "\n",
    "    Y_train = Y_train.reshape(Y_train.shape[0], 1)\n",
    "\n",
    "    return X_train.T, Y_train.T\n",
    "\n",
    "def load_test_data():\n",
    "    X_test = None\n",
    "    Y_test = None\n",
    "    pickleFile = unpickle('cifar-10-batches-py/test_batch')\n",
    "    dataX = pickleFile[b'data']\n",
    "    dataY = pickleFile[b'labels']\n",
    "    if type(X_test) is np.ndarray:\n",
    "        X_test = np.concatenate((X_test, dataX))\n",
    "        Y_test = np.concatenate((Y_test, dataY))\n",
    "    else:\n",
    "        X_test = np.array(dataX)\n",
    "        Y_test = np.array(dataY)\n",
    "\n",
    "    Y_test = Y_test.reshape(Y_test.shape[0], 1)\n",
    "\n",
    "    return X_test.T, Y_test.T\n",
    "\n",
    "def train_test_split(X_train, Y_train):\n",
    "    msk = np.random.rand(Y_train.shape[1]) < 0.8\n",
    "    X_Train = X_train[:,msk]  \n",
    "    X_val = X_train[:,~msk]\n",
    "    Y_Train = Y_train[:,msk]  \n",
    "    Y_val = Y_train[:,~msk]\n",
    "\n",
    "    return X_Train, Y_Train, X_val, Y_val\n",
    "\n",
    "def get_batch1(X, Y, batch_size):\n",
    "    n_batches = Y.shape[1]/batch_size\n",
    "    idx = np.arange(Y.shape[1])\n",
    "    np.random.shuffle(idx)\n",
    "    mini = np.array_split(idx, n_batches)\n",
    "\n",
    "    return mini\n",
    "\n",
    "def get_batch(X, Y, batch_size):\n",
    "    n_batches = int(Y.shape[1]/batch_size)\n",
    "    idx = np.arange(Y.shape[1])\n",
    "    np.random.shuffle(idx)\n",
    "    \n",
    "    ret = []\n",
    "    for i in range(0,int(len(idx)/batch_size)):\n",
    "        chunk = idx[i*batch_size:(i+1)*batch_size]\n",
    "        ret.append(chunk)\n",
    "    mini = ret\n",
    "    \n",
    "    return mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv1 = nn.Conv2d(3, 64, 5)\n",
    "        self.conv2 = nn.Conv2d(64, 256, 5)\n",
    "        self.fc1 = nn.Linear(256 * 5 * 5, 400)\n",
    "        self.fc2 = nn.Linear(400, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 256 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,Y_train = load_train_data()\n",
    "X_test, Y_test = load_test_data()\n",
    "X_Train, Y_Train, X_Val, Y_Val = train_test_split(X_train,Y_train)\n",
    "\n",
    "minn = np.min(X_Train, axis=1,keepdims=True)\n",
    "maxx = np.max(X_Train, axis=1,keepdims=True)\n",
    "X_Train= (X_Train - minn)/(maxx-minn)\n",
    "X_Val= (X_Val - minn)/(maxx-minn)\n",
    "X_Test= (X_test - minn)/(maxx-minn)\n",
    "\n",
    "net = Net().cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "def test_acc():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    idx = get_batch(X_Test, Y_test,100)\n",
    "\n",
    "    for idx_list in idx:\n",
    "        x_batch_test = [X_Test.T[index] for index in idx_list]\n",
    "        y_batch_test = [Y_test.T[index] for index in idx_list]\n",
    "        x_batch_test = np.asarray(x_batch_test)\n",
    "        y_batch_test1 = np.asarray(y_batch_test)\n",
    "        y_batch_onehot_test = get_one_hot(y_batch_test1,10)\n",
    "        label_tensor_test = torch.from_numpy(y_batch_onehot_test)\n",
    "        test_labels = Variable(label_tensor_test.cuda()).long()\n",
    "        true_labels = torch.max(test_labels,1)[1]\n",
    "        x_batch_test = x_batch_test.reshape(x_batch_test.shape[0],3,32,32)    \n",
    "        input_tensor_test = torch.from_numpy(x_batch_test)\n",
    "        images = Variable(input_tensor_test.cuda()).float()\n",
    "        outputs = net(images)\n",
    "        ##### loss = criterion(outputs, torch.max(labels, 1)[1])\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == true_labels.data.long()).sum()\n",
    "    \n",
    "    return (100 * correct / total)\n",
    "\n",
    "def get_one_hot(targets, nb_classes):\n",
    "    return np.eye(nb_classes)[np.array(targets).reshape(-1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n",
      "Epoch 1 Count: 300 loss: 2.248557195663452 \t 98 loss_val: 2.0844348287582397 Test: 24.29\n",
      "Epoch 2 Count: 300 loss: 1.991370896100998 \t 98 loss_val: 1.9619680261611938 Test: 28.79\n",
      "Epoch 3 Count: 300 loss: 1.866954118013382 \t 98 loss_val: 1.806718019247055 Test: 35.79\n",
      "Epoch 4 Count: 300 loss: 1.7662957656383513 \t 98 loss_val: 1.7242593741416932 Test: 38.99\n",
      "Epoch 5 Count: 300 loss: 1.6396108067035675 \t 98 loss_val: 1.592685316801071 Test: 42.15\n",
      "Epoch 6 Count: 300 loss: 1.5435718083381653 \t 98 loss_val: 1.5726390409469604 Test: 43.48\n",
      "Epoch 7 Count: 300 loss: 1.5015614902973176 \t 98 loss_val: 1.5758461904525758 Test: 43.43\n",
      "Epoch 8 Count: 300 loss: 1.454566730260849 \t 98 loss_val: 1.4768439221382141 Test: 47.64\n",
      "Epoch 9 Count: 300 loss: 1.4162591445446013 \t 98 loss_val: 1.385925953388214 Test: 50.42\n",
      "Epoch 10 Count: 300 loss: 1.3688940501213074 \t 98 loss_val: 1.366756227016449 Test: 50.74\n",
      "Epoch 11 Count: 300 loss: 1.3276127421855926 \t 98 loss_val: 1.3275480592250823 Test: 52.8\n",
      "Epoch 12 Count: 300 loss: 1.2727774739265443 \t 98 loss_val: 1.2986892008781432 Test: 53.66\n",
      "Epoch 13 Count: 300 loss: 1.2737469673156738 \t 98 loss_val: 1.2738651382923125 Test: 54.44\n",
      "Epoch 14 Count: 300 loss: 1.2523769915103913 \t 98 loss_val: 1.2429724776744842 Test: 56.12\n",
      "Epoch 15 Count: 300 loss: 1.2092142796516419 \t 98 loss_val: 1.2368769323825837 Test: 55.69\n",
      "Epoch 16 Count: 300 loss: 1.1744939762353896 \t 98 loss_val: 1.1763914358615875 Test: 58.64\n",
      "Epoch 17 Count: 300 loss: 1.127887127995491 \t 98 loss_val: 1.2269655579328538 Test: 55.81\n",
      "Epoch 18 Count: 300 loss: 1.1120202016830445 \t 98 loss_val: 1.132629196047783 Test: 59.99\n",
      "Epoch 19 Count: 300 loss: 1.1085664117336274 \t 98 loss_val: 1.1200312799215317 Test: 60.57\n",
      "Epoch 20 Count: 300 loss: 1.058249779343605 \t 98 loss_val: 1.1627606397867203 Test: 58.89\n",
      "Epoch 21 Count: 300 loss: 1.045036495923996 \t 98 loss_val: 1.1014732044935227 Test: 61.58\n",
      "Epoch 22 Count: 300 loss: 1.0311172771453858 \t 98 loss_val: 1.1080327838659287 Test: 60.77\n",
      "Epoch 23 Count: 300 loss: 1.0082794338464738 \t 98 loss_val: 1.069368600845337 Test: 62.84\n",
      "Epoch 24 Count: 300 loss: 0.9658566838502884 \t 98 loss_val: 1.0702710115909577 Test: 62.09\n",
      "Epoch 25 Count: 300 loss: 0.9684613335132599 \t 98 loss_val: 1.0527119344472886 Test: 63.5\n",
      "Epoch 26 Count: 300 loss: 0.9280754959583283 \t 98 loss_val: 1.048517125248909 Test: 63.66\n",
      "Epoch 27 Count: 300 loss: 0.914626150727272 \t 98 loss_val: 1.0660278522968292 Test: 62.54\n",
      "Epoch 28 Count: 300 loss: 0.9024392956495285 \t 98 loss_val: 1.0304362213611602 Test: 64.76\n",
      "Epoch 29 Count: 300 loss: 0.8931241273880005 \t 98 loss_val: 0.9882241451740265 Test: 66.06\n",
      "Epoch 30 Count: 300 loss: 0.8451444655656815 \t 98 loss_val: 0.9769697672128678 Test: 66.18\n",
      "Epoch 31 Count: 300 loss: 0.8292976248264313 \t 98 loss_val: 1.0317771208286286 Test: 64.8\n",
      "Epoch 32 Count: 300 loss: 0.8177258932590484 \t 98 loss_val: 0.9584578341245651 Test: 66.9\n",
      "Epoch 33 Count: 300 loss: 0.771534932255745 \t 98 loss_val: 0.9743635147809983 Test: 65.99\n",
      "Epoch 34 Count: 300 loss: 0.7810680395364762 \t 98 loss_val: 0.9959921681880951 Test: 65.86\n",
      "Epoch 35 Count: 300 loss: 0.7526245558261871 \t 98 loss_val: 1.0627988129854202 Test: 63.28\n",
      "Epoch 36 Count: 300 loss: 0.7226263552904129 \t 98 loss_val: 0.9625094354152679 Test: 66.89\n",
      "Epoch 37 Count: 300 loss: 0.6995393508672714 \t 98 loss_val: 0.9727744388580323 Test: 66.52\n",
      "Epoch 38 Count: 300 loss: 0.6770901307463646 \t 98 loss_val: 0.9610534298419953 Test: 67.12\n",
      "Epoch 39 Count: 300 loss: 0.6646863344311714 \t 98 loss_val: 0.9592359977960586 Test: 67.4\n",
      "Epoch 40 Count: 300 loss: 0.638387815952301 \t 98 loss_val: 0.9294513946771622 Test: 68.22\n",
      "Epoch 41 Count: 300 loss: 0.6289389616250992 \t 98 loss_val: 0.9419126391410828 Test: 67.4\n",
      "Epoch 42 Count: 300 loss: 0.6153305611014366 \t 98 loss_val: 0.9254469895362853 Test: 68.54\n",
      "Epoch 43 Count: 300 loss: 0.5775334939360619 \t 98 loss_val: 0.9389279967546463 Test: 68.76\n",
      "Epoch 44 Count: 300 loss: 0.5530884861946106 \t 98 loss_val: 1.0058342659473418 Test: 65.85\n",
      "Epoch 45 Count: 300 loss: 0.5274307355284691 \t 98 loss_val: 0.9645430433750153 Test: 67.34\n",
      "Epoch 46 Count: 300 loss: 0.508912943303585 \t 98 loss_val: 0.9380611664056778 Test: 68.6\n",
      "Epoch 47 Count: 300 loss: 0.49682989925146104 \t 98 loss_val: 0.9405972278118133 Test: 69.1\n",
      "Epoch 48 Count: 300 loss: 0.4685490587353706 \t 98 loss_val: 0.9391711044311524 Test: 69.27\n",
      "Epoch 49 Count: 300 loss: 0.44989392697811126 \t 98 loss_val: 0.9425098764896392 Test: 69.03\n",
      "Epoch 50 Count: 300 loss: 0.4170515649020672 \t 98 loss_val: 0.9835726130008697 Test: 68.46\n",
      "Epoch 51 Count: 300 loss: 0.4042532554268837 \t 98 loss_val: 0.9993800508975983 Test: 68.54\n",
      "Epoch 52 Count: 300 loss: 0.3790571519732475 \t 98 loss_val: 0.950272051692009 Test: 69.4\n",
      "Epoch 53 Count: 300 loss: 0.363659245967865 \t 98 loss_val: 0.9456435436010361 Test: 70.26\n",
      "Epoch 54 Count: 300 loss: 0.3541046340763569 \t 98 loss_val: 1.01059436917305 Test: 68.81\n",
      "Epoch 55 Count: 300 loss: 0.33254894584417344 \t 98 loss_val: 1.0586814498901367 Test: 68.28\n",
      "Epoch 56 Count: 300 loss: 0.30744568794965743 \t 98 loss_val: 1.1334568285942077 Test: 66.61\n",
      "Epoch 57 Count: 300 loss: 0.28598926693201066 \t 98 loss_val: 1.0454970163106918 Test: 69.27\n",
      "Epoch 58 Count: 300 loss: 0.2873576885461807 \t 98 loss_val: 1.0853533691167832 Test: 68.94\n",
      "Epoch 59 Count: 300 loss: 0.25489106714725496 \t 98 loss_val: 1.0797140550613404 Test: 68.89\n",
      "Epoch 60 Count: 300 loss: 0.2380563848465681 \t 98 loss_val: 1.0729278403520583 Test: 69.02\n",
      "Epoch 61 Count: 300 loss: 0.21301808312535286 \t 98 loss_val: 1.0990863507986068 Test: 69.06\n",
      "Epoch 62 Count: 300 loss: 0.1980396993458271 \t 98 loss_val: 1.1076353722810746 Test: 69.12\n",
      "Epoch 63 Count: 300 loss: 0.1808151388168335 \t 98 loss_val: 1.186525132060051 Test: 68.33\n",
      "Epoch 64 Count: 300 loss: 0.1811346873641014 \t 98 loss_val: 1.1996808862686157 Test: 67.8\n",
      "Epoch 65 Count: 300 loss: 0.1535955138504505 \t 98 loss_val: 1.1535380268096924 Test: 69.43\n",
      "Epoch 66 Count: 300 loss: 0.1670798510313034 \t 98 loss_val: 1.20023497402668 Test: 69.23\n",
      "Epoch 67 Count: 300 loss: 0.12675800275057555 \t 98 loss_val: 1.1989794903993607 Test: 69.26\n",
      "Epoch 68 Count: 300 loss: 0.13015876714140176 \t 98 loss_val: 1.2212359249591827 Test: 69.31\n",
      "Epoch 69 Count: 300 loss: 0.08424125507473945 \t 98 loss_val: 1.2051916098594666 Test: 69.5\n",
      "Epoch 70 Count: 300 loss: 0.08034130439162254 \t 98 loss_val: 1.2439815294742584 Test: 70.0\n",
      "Epoch 71 Count: 300 loss: 0.06866166584193706 \t 98 loss_val: 1.2661271840333939 Test: 69.84\n",
      "Epoch 72 Count: 300 loss: 0.06494406616315246 \t 98 loss_val: 1.278440034389496 Test: 69.46\n",
      "Epoch 73 Count: 300 loss: 0.05618930770084262 \t 98 loss_val: 1.3255275619029998 Test: 69.42\n",
      "Epoch 74 Count: 300 loss: 0.04531756324693561 \t 98 loss_val: 1.3333874267339707 Test: 69.75\n",
      "Epoch 75 Count: 300 loss: 0.049669070206582545 \t 98 loss_val: 1.3101673430204392 Test: 69.91\n",
      "Epoch 76 Count: 300 loss: 0.0381401302292943 \t 98 loss_val: 1.3596543139219284 Test: 69.8\n",
      "Epoch 77 Count: 300 loss: 0.03158733010292053 \t 98 loss_val: 1.4625210016965866 Test: 68.5\n",
      "Epoch 78 Count: 300 loss: 0.02926346065476537 \t 98 loss_val: 1.3957543408870696 Test: 69.78\n",
      "Epoch 79 Count: 300 loss: 0.024998530028387905 \t 98 loss_val: 1.4076738667488098 Test: 70.1\n",
      "Epoch 80 Count: 300 loss: 0.024375513484701513 \t 98 loss_val: 1.4106465405225754 Test: 69.76\n"
     ]
    }
   ],
   "source": [
    "trainacc=[]\n",
    "valacc=[]\n",
    "testacc = []\n",
    "traincost = []\n",
    "valcost = []\n",
    "epoch_count = 200\n",
    "num = epoch_count\n",
    "batch_size_len = 100 \n",
    "\n",
    "train = \"\"\n",
    "print(\"Start\", flush=True)\n",
    "for epoch in range(num):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    running_loss_val = 0.0\n",
    "    train_accuracy = 0.0\n",
    "    val_accuracy = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    #print(epoch, flush=True)\n",
    "    abcd_train = get_batch(X_Train, Y_Train,batch_size_len)\n",
    "    for i, idx in enumerate(abcd_train, 0):\n",
    "        # get the inputs\n",
    "        x_batch = [X_Train.T[index] for index in idx]\n",
    "        y_batch = [Y_Train.T[index] for index in idx]\n",
    "        x_batch = np.asarray(x_batch)\n",
    "        y_batch = np.asarray(y_batch)\n",
    "        y_batch_onehot = get_one_hot(y_batch,10)\n",
    "        x_batch = x_batch.reshape(x_batch.shape[0],3,32,32)\n",
    "        input_tensor = torch.from_numpy(x_batch)\n",
    "        label_tensor = torch.from_numpy(y_batch_onehot)\n",
    "        inputs = Variable(input_tensor.cuda()).float()\n",
    "        labels = Variable(label_tensor.cuda()).long()\n",
    "        true_labels = torch.max(labels,1)[1]\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, torch.max(labels, 1)[1])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.data[0]\n",
    "        if i % 100 == 99:    # print every 2000 mini-batches\n",
    "            if(i+1 == 300):\n",
    "                #print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 100), flush=True)\n",
    "                #train = train + str(epoch + 1) + \", \" + str(i + 1) + \", loss=\" + str(running_loss / 100) + \"; \"\n",
    "                print(\"Epoch\", epoch + 1, \"Count:\", i + 1, \"loss:\", running_loss/(batch_size_len*1.0), end=' ')\n",
    "            traincost.append(running_loss / (batch_size_len*1.0))\n",
    "            running_loss = 0.0\n",
    "            \n",
    "        _, predicted_train = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted_train == true_labels.data.long()).sum()\n",
    "    train_accuracy = 100 * correct_train / (total_train + 0.0001)\n",
    "        \n",
    "    abcd = get_batch(X_Val, Y_Val,batch_size_len)\n",
    "#     for l in abcd:\n",
    "#         print(len(l), end=' ')\n",
    "#     print(\"\")\n",
    "    for i, idx in enumerate(abcd, 0):\n",
    "        # get the inputs\n",
    "        x_batch_val = [X_Val.T[index] for index in idx]\n",
    "        y_batch_val = [Y_Val.T[index] for index in idx]\n",
    "        x_batch_val = np.asarray(x_batch_val)\n",
    "        y_batch_val = np.asarray(y_batch_val)\n",
    "        y_batch_onehot_val = get_one_hot(y_batch_val,10)\n",
    "        x_batch_val = x_batch_val.reshape(x_batch_val.shape[0],3,32,32)\n",
    "        input_tensor_val = torch.from_numpy(x_batch_val)\n",
    "        label_tensor_val = torch.from_numpy(y_batch_onehot_val)\n",
    "        inputs_val = Variable(input_tensor_val.cuda()).float()\n",
    "        labels_val = Variable(label_tensor_val.cuda()).long()\n",
    "        true_labels_val = torch.max(labels_val,1)[1]\n",
    "        optimizer.zero_grad()\n",
    "        outputs_val = net(inputs_val)\n",
    "        loss_val = criterion(outputs_val, torch.max(labels_val, 1)[1])\n",
    "        running_loss_val += loss_val.data[0]\n",
    "#         if(i > 97):\n",
    "#             print(\"Hello\", i)\n",
    "        if i == len(abcd) - 1:    # print every 2000 mini-batches\n",
    "            test_accuracy = test_acc()\n",
    "            print(\"\\t\", i, \"loss_val:\", running_loss_val/(batch_size_len*1.0), \"Test:\", test_accuracy)\n",
    "            valcost.append(running_loss_val/(batch_size_len*1.0))\n",
    "            running_loss_val = 0.0\n",
    "        \n",
    "        _, predicted_val = torch.max(outputs_val.data, 1)\n",
    "        total_val += labels_val.size(0)\n",
    "        correct_val += (predicted_val == true_labels_val.data.long()).sum()\n",
    "    val_accuracy = 100 * correct_val / total_val\n",
    "    trainacc.append(train_accuracy)\n",
    "    valacc.append(val_accuracy)\n",
    "    testacc.append(test_accuracy)\n",
    "    if(val_accuracy>=70.2):\n",
    "        print(\"Target Val Accuracy reached. Breaking\", flush=True)\n",
    "        break\n",
    "    \n",
    "print('Finished Training', flush=True)\n",
    "print('Training Accuracy ',trainacc, flush=True)\n",
    "print('validation Accuracy',valacc, flush=True)\n",
    "print(\"Test:\", test_acc(), flush=True)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(trainacc, 'r', label ='train-accuracy')\n",
    "plt.plot(valacc, 'b', label = 'val-accuracy')\n",
    "plt.plot(testacc, 'g', label = 'test-accuracy')\n",
    "plt.legend()\n",
    "plt.savefig(\"test.png\")\n",
    "\n",
    "\n",
    "x_t = []\n",
    "x_v = []\n",
    "\n",
    "for i in range(len(traincost)):\n",
    "    x_t.append((i+1)*batch_size_len)\n",
    "\n",
    "for i in range(len(valcost)):\n",
    "    x_v.append((i+1)*batch_size_len)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x_t,traincost, 'b', label = 'traincost')\n",
    "plt.plot(x_v,valcost, 'r', label = 'valcost')\n",
    "plt.legend()\n",
    "plt.savefig(\"test1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
